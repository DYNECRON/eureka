{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REad the stop words list\n",
    "stop_words_paht='data/reuters/stopwords'\n",
    "stop_words=[]\n",
    "with open(stop_words_paht,'r') as file:\n",
    "    content=file.read().split('\\n')\n",
    "    stop_words.append(content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = 'data/reuters/training/'\n",
    "filenames=os.listdir(data_directory)\n",
    "filenames.sort(key=lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jeffersonc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "stop_words_path = 'data/reuters/stopwords'\n",
    "data_directory = 'data/reuters/training/'\n",
    "filenames = os.listdir(data_directory)\n",
    "filenames.sort(key=lambda x: int(x))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('stopwords')\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "processed_doc = {}\n",
    "real_doc = {}\n",
    "\n",
    "def load_data():\n",
    "    for filename in filenames:\n",
    "        with open(os.path.join(data_directory, filename), 'r') as file:\n",
    "            content = file.read()\n",
    "            real_doc[filename] = content\n",
    "            content = normalize_text(content)\n",
    "            words_list = []\n",
    "            for word in content.translate(str.maketrans('', '', string.punctuation)).split(\" \"):\n",
    "                if word not in stop_words[0] or word not in stop_words_nltk:\n",
    "                    word = lemmatizer.lemmatize(word)\n",
    "                    words_list.append(word)\n",
    "            lemmatized_doc = \" \".join(words_list)\n",
    "            processed_doc[filename] = lemmatized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data()\n",
    "filenames_list = []\n",
    "original_text_list = []\n",
    "processed_text = []\n",
    "\n",
    "# Iterar sobre cada nombre de archivo y sus correspondientes textos originales y procesados\n",
    "for filename, original_text in real_doc.items():\n",
    "    filenames_list.append(filename)\n",
    "    original_text_list.append(original_text)\n",
    "    processed_text.append(processed_doc[filename])\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'filename': filenames_list,\n",
    "    'original_text': original_text_list,\n",
    "    'processed_text': processed_text\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Creamos una instancia de CountVectorizer para BoW\n",
    "count_vectorizer = CountVectorizer(binary=True)\n",
    "# Creamos una instancia de CountVectorizer para TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "# Aplicar Bag of Words (BoW)\n",
    "bow_matrix = count_vectorizer.fit_transform(df['processed_text'])\n",
    "bow_feature_names = count_vectorizer.get_feature_names_out()\n",
    "# Aplicar TF-IDF \n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_text']) \n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"income\"  # Ejemplo de consulta\n",
    "query_bow = count_vectorizer.transform([query])  #vectorizamos al query para BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3394887447357178\n"
     ]
    }
   ],
   "source": [
    "#Calculamos la similituds jaccard usando la matriz de BoW\n",
    "def similitud_jaccard(query, text):\n",
    "    interseccion = np.sum(np.logical_and(query, text))\n",
    "    union = np.sum(np.logical_or(query, text))\n",
    "    jaccard_score = interseccion / union if union != 0 else 0.0\n",
    "    return jaccard_score\n",
    "\n",
    "import time\n",
    "start=time.time()\n",
    "jaccard_similarities1 = []\n",
    "for idx in range(bow_matrix.shape[0]):\n",
    "    \n",
    "    a=query_bow.toarray().squeeze()\n",
    "    b=bow_matrix[idx].toarray().squeeze()\n",
    "    similarity=similitud_jaccard(a,b)\n",
    "\n",
    "    jaccard_similarities1.append(similarity)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nos devuelve los indices de la lista de jaccard_similarities1\n",
    "sorted_indices1 = np.argsort(jaccard_similarities1)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto #7098 - Similitud Jaccard: 0.16666666666666666\n",
      "U.S. PERSONAL INCOME ROSE 0.9 PCT IN FEBRUARY, SPENDING UP 1.7 PCT\n",
      "\n",
      "  U.S. PERSONAL INCOME ROSE 0.9 PCT IN FEBRUARY, SPENDING UP 1.7 PCT\n",
      "  \n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "Texto #8016 - Similitud Jaccard: 0.08333333333333333\n",
      "MONTGOMERY STREET INCOME &lt;MTS> MONTHLY DIVIDEND\n",
      "  Mthly div 15 cts vs 15 cts\n",
      "      Pay April 15\n",
      "      Record April 1\n",
      "  \n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "Texto #508 - Similitud Jaccard: 0.07692307692307693\n",
      "&lt;FRANKLIN CALIFORNIA TAX-FREE INCOME FUND>PAYOUT\n",
      "  Mthly div 4.5 cts vs 4.5 cts prior\n",
      "      Pay March 13\n",
      "      Record March Two\n",
      "  \n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "Texto #512 - Similitud Jaccard: 0.07142857142857142\n",
      "&lt;FRANKLIN FEDERAL TAX-FREE INCOME FUND> PAYOUT\n",
      "  Mthly div 7.7 cts vs 7.7 cts prior\n",
      "      Pay March 13\n",
      "      Record March Two\n",
      "  \n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "Texto #513 - Similitud Jaccard: 0.06666666666666667\n",
      "&lt;FRANKLIN NEW YORK TAX-FREE INCOME FUND> PAYOUT\n",
      "  Mthly div 7.3 cts vs 7.3 cts prior\n",
      "      Pay March 13\n",
      "      Record March Two\n",
      "  \n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "Texto #690 - Similitud Jaccard: 0.06666666666666667\n",
      "FRANKLIN INSURED TAX-FREE SETS PAYOUT\n",
      "  Mthly div 7.1 cts vs 7.1 cts prior\n",
      "      Pay March 31\n",
      "      Record March 16\n",
      "      NOTE: Franklin Insured Tax-Free Income Fund.\n",
      "  \n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "Texto #509 - Similitud Jaccard: 0.06666666666666667\n",
      "&lt;FRANKLIN AGE HIGH INCOME FUND> SETS PAYOUT\n",
      "  Mthly div 3.6 cts vs 3.6 cts prior\n",
      "      Pay March 13\n",
      "      Record March Two\n",
      "  \n",
      "\n",
      "\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for idx in sorted_indices1:\n",
    "    filename=df['filename'].iloc[idx]\n",
    "    texto = df['original_text'].iloc[idx]\n",
    "    print(f\"Texto #{filename} - Similitud Jaccard: {jaccard_similarities1[idx]}\")\n",
    "    print(texto)\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    if i>5:\n",
    "        break\n",
    "    else:\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  income\n",
      "Texto #7098 - Distancia coseno: 0.4082482904638631\n",
      "U.S. PERSONAL INCOME ROSE 0.9 PCT IN FEBRUARY, SPENDING UP 1.7 PCT\n",
      "\n",
      "  U.S. PERSONAL INCOME ROSE 0.9 PCT IN FEBRUARY, SPENDING UP 1.7 PCT\n",
      "  \n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "Texto #8016 - Distancia coseno: 0.2886751345948129\n",
      "MONTGOMERY STREET INCOME &lt;MTS> MONTHLY DIVIDEND\n",
      "  Mthly div 15 cts vs 15 cts\n",
      "      Pay April 15\n",
      "      Record April 1\n",
      "  \n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "Texto #508 - Distancia coseno: 0.2773500981126146\n",
      "&lt;FRANKLIN CALIFORNIA TAX-FREE INCOME FUND>PAYOUT\n",
      "  Mthly div 4.5 cts vs 4.5 cts prior\n",
      "      Pay March 13\n",
      "      Record March Two\n",
      "  \n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "Texto #512 - Distancia coseno: 0.2672612419124244\n",
      "&lt;FRANKLIN FEDERAL TAX-FREE INCOME FUND> PAYOUT\n",
      "  Mthly div 7.7 cts vs 7.7 cts prior\n",
      "      Pay March 13\n",
      "      Record March Two\n",
      "  \n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "Texto #513 - Distancia coseno: 0.2581988897471611\n",
      "&lt;FRANKLIN NEW YORK TAX-FREE INCOME FUND> PAYOUT\n",
      "  Mthly div 7.3 cts vs 7.3 cts prior\n",
      "      Pay March 13\n",
      "      Record March Two\n",
      "  \n",
      "\n",
      "\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query_tfidf =tfidf_vectorizer.transform([query]) #vectorizamos a tf-idf la query\n",
    "\n",
    "# Cosine similarity using numpy\n",
    "def cosine_sim(a,b):\n",
    "    if np.linalg.norm(a)*np.linalg.norm(b)>0.0:        \n",
    "        return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "cosine_distances_list = []\n",
    "for idx in range(bow_matrix.shape[0]):\n",
    "    distance=cosine_sim(bow_matrix[idx].toarray().squeeze(),query_tfidf.toarray().squeeze())\n",
    "    \n",
    "    cosine_distances_list.append(distance)\n",
    "    \n",
    "    \n",
    "sorted_indices_cos = np.argsort(cosine_distances_list)[::-1]\n",
    "\n",
    "\n",
    "i=0\n",
    "print(\"Query: \",query)\n",
    "for i, idx2 in enumerate(sorted_indices_cos):\n",
    "    if (cosine_distances_list[idx2]>0.0 and i<5):\n",
    "        filename=df['filename'].iloc[idx2]\n",
    "        texto = df['original_text'].iloc[idx2]\n",
    "        print(f\"Texto #{filename} - Distancia coseno: {cosine_distances_list[idx2]}\")\n",
    "        print(texto)\n",
    "        print(\"-----------------------------------------------------\")\n",
    "    else:\n",
    "        break\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_resultados = 'data/reuters/cats.txt'\n",
    "\n",
    "consultas_resultados = {}\n",
    "\n",
    "with open(archivo_resultados, 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if line.startswith('training'):\n",
    "            parts = line.split()\n",
    "            archivo = parts[0]\n",
    "            archivo= archivo.split('/')[1].strip().split()[0]\n",
    "            consulta = ' '.join(parts[1:])\n",
    "            if consulta in consultas_resultados:\n",
    "                consultas_resultados[consulta].add(archivo)\n",
    "            else:\n",
    "                consultas_resultados[consulta] = {archivo}\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_vectorBow(query):\n",
    "    return count_vectorizer.transform([query])\n",
    "    \n",
    "    \n",
    "def similitud_jaccard(query, text):\n",
    "    interseccion = np.sum(np.logical_and(query, text))\n",
    "    union = np.sum(np.logical_or(query, text))\n",
    "    jaccard_score = interseccion / union if union != 0 else 0.0\n",
    "    return jaccard_score\n",
    "\n",
    "\n",
    "def calculate_jaccard(query):\n",
    "    #start=time.time()\n",
    "    jaccard_similarities1 = []\n",
    "    best_titles_jaccard=[]\n",
    "    query_bow=query_to_vectorBow(query)\n",
    "    for idx in range(bow_matrix.shape[0]):        \n",
    "        a=query_bow.toarray().squeeze()\n",
    "        b=bow_matrix[idx].toarray().squeeze()\n",
    "        similarity=similitud_jaccard(a,b)\n",
    "\n",
    "        jaccard_similarities1.append(similarity)\n",
    "    sorted_indices = np.argsort(jaccard_similarities1)[::-1]\n",
    "    for idx in sorted_indices:\n",
    "        if jaccard_similarities1[idx]>0.0:\n",
    "            filename=df['filename'].iloc[idx]\n",
    "            best_titles_jaccard.append(filename)\n",
    "    return best_titles_jaccard\n",
    "        \n",
    "    #end=time.time()\n",
    "    #print(end-start)\n",
    "    \n",
    "\n",
    "def get_true_positives(predicted, truth):\n",
    "    true_positives = 0\n",
    "    for value in predicted:\n",
    "        if value in truth:\n",
    "            true_positives += 1\n",
    "\n",
    "    #print(\"True positives\", true_positives)\n",
    "    return true_positives\n",
    "\n",
    "\n",
    "def get_false_negatives(predicted, truth):\n",
    "    set_predicted = set(predicted)\n",
    "    set_verdaderos = set(truth)\n",
    "    false_negatives_list = list(set_verdaderos-set_predicted)\n",
    "    false_negatives = len(false_negatives_list)\n",
    "\n",
    "    #print(\"False negatives\", false_negatives)\n",
    "    return false_negatives\n",
    "\n",
    "\n",
    "def get_false_positive(predicted, truth):\n",
    "    set_predicted = set(predicted)\n",
    "    set_verdaderos = set(truth)\n",
    "    false_positives_list = list(set_predicted-set_verdaderos)\n",
    "    false_positives = len(false_positives_list)\n",
    "    #print(\"False positives\", false_positives)\n",
    "\n",
    "    return false_positives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de Recall jaccard: 0.8378790622708628\n",
      "Promedio de Precisión jaccard: 0.03297630019929637\n",
      "Promedio f1 score jaccard: 0.04514864039827736\n"
     ]
    }
   ],
   "source": [
    "#Para jaccard usando BoW\n",
    "recall_results=[]\n",
    "precision_results=[]\n",
    "f1_score_results=[]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for key in consultas_resultados:\n",
    "    truth=consultas_resultados[key]\n",
    "    key_lematized=lemmatizer.lemmatize(key)\n",
    "    best_titles_jaccard=calculate_jaccard(key_lematized)\n",
    "    true_positives=get_true_positives(best_titles_jaccard,truth)\n",
    "    false_negatives=get_false_negatives(best_titles_jaccard,truth)\n",
    "    false_positives=get_false_positive(best_titles_jaccard,truth)\n",
    "    if true_positives + false_negatives == 0:\n",
    "        recall = 0  \n",
    "    else:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "    \n",
    "    recall_results.append(recall)\n",
    "    \n",
    "    if true_positives + false_positives == 0:\n",
    "        precision = 0  \n",
    "    else:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "    \n",
    "    precision_results.append(precision)\n",
    "    \n",
    "    if ((2*true_positives)+false_negatives+false_positives)==0:\n",
    "        f1_score=0\n",
    "    else:\n",
    "        f1_score=(2*true_positives)/((2*true_positives)+false_negatives+false_positives)\n",
    "    f1_score_results.append(f1_score)\n",
    "    \n",
    "\n",
    "promedio_recall=sum(recall_results)/len(recall_results)\n",
    "promedio_precision=sum(precision_results)/len(precision_results)    \n",
    "promedio_f1_score=sum(f1_score_results)/len(f1_score_results)\n",
    "\n",
    "print(f\"Promedio de Recall jaccard: {promedio_recall}\")\n",
    "print(f\"Promedio de Precisión jaccard: {promedio_precision}\")\n",
    "print(f\"Promedio f1 score jaccard: {promedio_f1_score}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity using numpy\n",
    "def cosine_sim(a,b):\n",
    "    if np.linalg.norm(a)*np.linalg.norm(b)>0.0:        \n",
    "        return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "    \n",
    "def calculate_cosine(query):\n",
    "    #start=time.time()\n",
    "    distances = []\n",
    "    best_titles_cosine=[]\n",
    "    query_bow=query_to_vectorBow(query)\n",
    "    for idx in range(bow_matrix.shape[0]):        \n",
    "        a=query_bow.toarray().squeeze()\n",
    "        b=bow_matrix[idx].toarray().squeeze()\n",
    "        similarity=cosine_sim(a,b)\n",
    "\n",
    "        distances.append(similarity)\n",
    "    sorted_indices = np.argsort(jaccard_similarities1)[::-1]\n",
    "    for idx in sorted_indices:\n",
    "        if distances[idx]>0.0:\n",
    "            filename=df['filename'].iloc[idx]\n",
    "            best_titles_cosine.append(filename)\n",
    "    return best_titles_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para TF-IDF con cosine\n",
    "recall_results=[]\n",
    "precision_results=[]\n",
    "f1_score_results=[]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for key in consultas_resultados:\n",
    "    truth=consultas_resultados[key]\n",
    "    key_lematized=lemmatizer.lemmatize(key)\n",
    "    best_titles=calculate_cosine(key_lematized)\n",
    "    true_positives=get_true_positives(best_titles,truth)\n",
    "    false_negatives=get_false_negatives(best_titles,truth)\n",
    "    false_positives=get_false_positive(best_titles,truth)\n",
    "    if true_positives + false_negatives == 0:\n",
    "        recall = 0  \n",
    "    else:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "    \n",
    "    recall_results.append(recall)\n",
    "    \n",
    "    if true_positives + false_positives == 0:\n",
    "        precision = 0  \n",
    "    else:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "    \n",
    "    precision_results.append(precision)\n",
    "    \n",
    "    if ((2*true_positives)+false_negatives+false_positives)==0:\n",
    "        f1_score=0\n",
    "    else:\n",
    "        f1_score=(2*true_positives)/((2*true_positives)+false_negatives+false_positives)\n",
    "    f1_score_results.append(f1_score)\n",
    "    \n",
    "\n",
    "promedio_recall=sum(recall_results)/len(recall_results)\n",
    "promedio_precision=sum(precision_results)/len(precision_results)    \n",
    "promedio_f1_score=sum(f1_score_results)/len(f1_score_results)\n",
    "\n",
    "print(f\"Promedio de Recall cosine: {promedio_recall}\")\n",
    "print(f\"Promedio de Precisión cosine: {promedio_precision}\")\n",
    "print(f\"Promedio f1 score cosine: {promedio_f1_score}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
